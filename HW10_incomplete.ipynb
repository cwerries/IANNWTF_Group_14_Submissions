{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW10_incomplete.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMHQGa1Zuy2OBV2gYlFi6HO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is an unfinished version because we were sooooo confused \n"
      ],
      "metadata": {
        "id": "-eqRhH0aT_5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U tensorflow-text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d22pYmdB2VuE",
        "outputId": "000b1712-9132-4158-c3e2-39e554d009a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.9 MB 5.1 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jRISfTg3BkJT"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "import regex as re\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# b a s h code t o mount t h e d r i v e\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('drive/MyDrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "v-xLjmi9v-xL",
        "outputId": "92f94e82-b8d6-4afe-a69b-feab851d15e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d00c63f11cdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drive/MyDrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m       \u001b[0muse_metadata_server\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_metadata_server\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m       ephemeral=ephemeral)\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server, ephemeral)\u001b[0m\n\u001b[1;32m    134\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 136\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "file = open(\"Colab/bible.txt\").read()\n"
      ],
      "metadata": {
        "id": "-8wu6PwEN3Mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Colab/bible.txt\") as myfile:\n",
        "    head = [next(myfile) for x in range(10)]\n",
        "head = str(head)"
      ],
      "metadata": {
        "id": "W-zPVUgrWLcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Head = small data for testing\n",
        "file = whole data"
      ],
      "metadata": {
        "id": "92Rc1pgikmL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(data):\n",
        "    data = data.lower()\n",
        "    data = data.replace(\"\\\\n\", \"\")\n",
        "    data = re.sub('[^A-Za-z0-9 ]+', '', data)\n",
        "    splitter = tf_text.UnicodeScriptTokenizer()\n",
        "    data = splitter.tokenize(data)\n",
        "    return data\n",
        "\n",
        "def smaler_text(data, vocab_size):\n",
        "    words_counts = Counter(data).most_common(vocab_size)\n",
        "    word2id = {word: word_id for word_id, (word, _) in enumerate(words_and_count, 1)}\n",
        "    word2id[\"_UNKNOWN_\"] = 0\n",
        "    id2word = dict(zip(word2id.values(), word2id.keys()))\n",
        "    return word2id, id2word\n",
        "\n",
        "\n",
        "bible_id = [word2id.get(word, 0) for word in bible_text_tokenized]"
      ],
      "metadata": {
        "id": "zynPXevvizSn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "460f5cbf-bd70-4740-96a4-a929fedcc371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-cf5ee43a7441>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mbible_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword2id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbible_text_tokenized\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'bible_text_tokenized' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n"
      ],
      "metadata": {
        "id": "lie6EG4EJYqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare(data, batch_size):\n",
        "\n",
        "    \"\"\"\n",
        "    input = str data\n",
        "    \n",
        "    \"\"\"\n",
        "    data = data.lower()\n",
        "    data = data.replace(\"\\\\n\", \"\")\n",
        "    data = re.sub('[^A-Za-z0-9 ]+', '', data)\n",
        "    splitter = tf_text.UnicodeScriptTokenizer()\n",
        "    data = splitter.tokenize(data)\n",
        "\n",
        "    # unique values, idx, number of occurance\n",
        "    unique_data_with_counts = tf.unique_with_counts(data)\n",
        "\n",
        "    # give the 10 most occuring words\n",
        "    most_common_id = tf.argsort(unique_data_with_counts[2], direction=\"DESCENDING\")[:10] # 10000\n",
        "    most_common_words = tf.gather(unique_data_with_counts[0],most_common_id)\n",
        "\n",
        "    for idx in range(data):\n",
        "        if data[i] in most_common_words:\n",
        "            target = \n",
        "    # give data only with common words\n",
        "    #small_data = [i for i in data if i in most_common_words]\n",
        "    #text = ' '.join([word for word in data.split() if word not in most_common_words])\n",
        "    #print(\"\\n\",\"small\",text)\n",
        "\n",
        "    # create input- target pairs\n",
        "\n",
        "    return data, most_common_words,most_common_id\n",
        "\n",
        "prepare(head, 8),"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xScvkBwgJXWc",
        "outputId": "7b430cbe-2b48-45b9-9ff6-f23acc9846c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((<tf.Tensor: shape=(60,), dtype=string, numpy=\n",
              "  array([b'the', b'first', b'book', b'of', b'moses', b'called', b'genesis',\n",
              "         b'11', b'in', b'the', b'beginning', b'god', b'created', b'the',\n",
              "         b'heaven', b'and', b'the', b'earth', b'12', b'and', b'the',\n",
              "         b'earth', b'was', b'without', b'form', b'and', b'void', b'and',\n",
              "         b'darkness', b'was', b'upon', b'the', b'face', b'of', b'the',\n",
              "         b'deep', b'and', b'the', b'spirit', b'of', b'god', b'moved',\n",
              "         b'upon', b'the', b'face', b'of', b'the', b'waters', b'13', b'and',\n",
              "         b'god', b'said', b'let', b'there', b'be', b'light', b'and',\n",
              "         b'there', b'was', b'light'], dtype=object)>,\n",
              "  <tf.Tensor: shape=(10,), dtype=string, numpy=\n",
              "  array([b'the', b'and', b'of', b'god', b'was', b'earth', b'upon', b'face',\n",
              "         b'there', b'light'], dtype=object)>),)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_pairs(data, word_list = 5):"
      ],
      "metadata": {
        "id": "jnnGaH-7uD4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGram(tf.keras.layers.Layers):\n",
        "    def __init__(self, vocab_size, embedding_size):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "        self.embed\n",
        "\n",
        "    def build(self, input_shape)\n",
        "         self.emb_matrix = self.add_weight(\n",
        "             shape=(self.vocab_size, self.embedding_size),\n",
        "             initializer='GlorotNormal'\n",
        "             )\n",
        "        self.score_matrix = self.add_weight(\n",
        "            shape=(self.vocab_size, self.embedding_size),\n",
        "            initializer='GlorotNormal'\n",
        "            )\n",
        "        self.score_bias = self.add_weight(\n",
        "            shape=(self.vocab_size),\n",
        "            initializer='zeros'\n",
        "            )\n",
        "\n",
        "    def call(self, input, labels):\n",
        "        embeddings = tf.nn.embedding_lookup(self.emb_matrix,inputs)\n",
        "        nce_loss = tf.nn.nce_loss(\n",
        "            weights = self.score_matrix,\n",
        "            biases = self.score_bias,\n",
        "            labels = labels,\n",
        "            inputs = embeddings\n",
        "            )\n",
        "        loss = tf.reduce_mean(nce_loss)\n",
        "        return loss\n",
        "        \n",
        "\n"
      ],
      "metadata": {
        "id": "D-5NoWXVy1hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training():                                                    \n",
        "    # calculate and proint nearest neighbours(cosine)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "9rMGrDkj2dwU",
        "outputId": "a98e773c-1a67-4b63-ed31-c18801078cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-a46f0ae38ec0>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def training():\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 64\n",
        "k = 5\n",
        "check_words= [\"holy\", \"father\", \"wine\", \"poison\", \"love\",\n",
        "\"strong\", \"day\" ]\n",
        "\n",
        "def nearest_neighbours(check_words, k , embeddings):\n",
        "    for word in check_words:\n",
        "        word_embedding = \n",
        "        cosine_sim = -dot(embeddings, word_embedding)/(norm(embeddings)*norm(word_embedding))\n",
        "        nearest_neighbours = np.argsort(cosine_sim)[:k]\n",
        "        print(\"to Word \", word,\" the nearest neighbours are\",.join([id2word[n] for n in nearest_neighbours )"
      ],
      "metadata": {
        "id": "sGsrLI8t2fJn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}