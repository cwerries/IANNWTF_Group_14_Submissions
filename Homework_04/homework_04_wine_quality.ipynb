{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fedf8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#print(tf.test.gpu_device_name())\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=6000)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fe2c275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank wine into good or bad\n",
    "# function receiving a target and returning a binary value depending if target >= predefined threshold\n",
    "def make_binary(target, threshold):\n",
    "    if target >= tf.constant(threshold, dtype=tf.int32):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aef1a77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.hidden_layer1 = tf.keras.layers.Dense(32, activation=tf.nn.relu, kernel_regularizer= regularizers.l2(0.001))\n",
    "        self.hidden_layer2 = tf.keras.layers.Dense(32, activation=tf.nn.relu, kernel_regularizer= regularizers.l2(0.001))\n",
    "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "        self.output_layer = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training = False):\n",
    "        x = self.hidden_layer1(inputs)\n",
    "        x = self.hidden_layer2(x)\n",
    "        if training:\n",
    "            x = self.dropout(x, training=training)    \n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b84984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, input, target, loss_function, optimizer):\n",
    "    # loss_object and optimizer_object are instances of respective tensorflow classes\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = model(input)\n",
    "        loss = loss_function(target, prediction)\n",
    "        sample_train_accuracy =  target == np.round(prediction)\n",
    "        sample_train_accuracy = np.mean(sample_train_accuracy)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss, sample_train_accuracy\n",
    "\n",
    "def validate(model, val_data, loss_function):\n",
    "    # test over complete test data\n",
    "\n",
    "    val_accuracy_aggregator = []\n",
    "    val_loss_aggregator = []\n",
    "\n",
    "    for (input, target) in val_data:\n",
    "        prediction = model(input)\n",
    "        sample_val_loss = loss_function(target, prediction)\n",
    "        sample_val_accuracy =  target == np.round(prediction)\n",
    "        sample_val_accuracy = np.mean(sample_val_accuracy)\n",
    "        val_loss_aggregator.append(sample_val_loss.numpy())\n",
    "        val_accuracy_aggregator.append(np.mean(sample_val_accuracy))\n",
    "\n",
    "    val_loss = tf.reduce_mean(val_loss_aggregator)\n",
    "    val_accuracy = tf.reduce_mean(val_accuracy_aggregator)\n",
    "\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "def test(model, test_data, loss_function):\n",
    "    # test over complete test data\n",
    "\n",
    "    test_accuracy_aggregator = []\n",
    "    test_loss_aggregator = []\n",
    "\n",
    "    for (input, target) in test_data:\n",
    "        prediction = model(input)\n",
    "        sample_test_loss = loss_function(target, prediction)\n",
    "        sample_test_accuracy =  target == np.round(prediction)\n",
    "        sample_test_accuracy = np.mean(sample_test_accuracy)\n",
    "        test_loss_aggregator.append(sample_test_loss.numpy())\n",
    "        test_accuracy_aggregator.append(np.mean(sample_test_accuracy))\n",
    "\n",
    "    test_loss = tf.reduce_mean(test_loss_aggregator)\n",
    "    test_accuracy = tf.reduce_mean(test_accuracy_aggregator)\n",
    "\n",
    "    return test_loss, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "654a66a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize accuracy and loss for training and test data.\n",
    "def plotting(train_losses, test_losses, val_losses, train_accuracies, test_accuracies, val_accuracies):\n",
    "    # plot losses\n",
    "    plt.figure()\n",
    "    line1, = plt.plot(train_losses)\n",
    "    line2, = plt.plot(test_losses)\n",
    "    line3, = plt.plot(val_losses)\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"Training steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend((line1,line2,line3),(\"Loss train\",\"Loss test\",\"Loss validate\"))\n",
    "    plt.show()\n",
    "    \n",
    "    # plot accuracies\n",
    "    plt.figure()\n",
    "    line1, = plt.plot(train_accuracies)\n",
    "    line2, = plt.plot(test_accuracies)\n",
    "    line3, = plt.plot(val_accuracies)\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.xlabel(\"Training steps\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend((line1,line2,line3),(\"Accuracy train\", \"Accuracy test\",\"Accuracy validate\"))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c13259c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset, threshold):\n",
    "    # ranking wine in good and bad depending on threshold -> binary calssification\n",
    "    # Data Pipeline \n",
    "    \n",
    "    # map make_binary() function to dataset\n",
    "    dataset = dataset.map(lambda inputs, target: (inputs, make_binary(target, threshold)))\n",
    "  \n",
    "    # shuffle\n",
    "    dataset = dataset.shuffle(1599)\n",
    "    \n",
    "    # apply batching \n",
    "    dataset = dataset.batch(1599)\n",
    "\n",
    "    # prefetch data\n",
    "    dataset = dataset.prefetch(1599)\n",
    "\n",
    "    #return preprocessed dataset\n",
    "    return dataset  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e88979",
   "metadata": {},
   "source": [
    "## Preparation of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddabaf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into a data frame\n",
    "df_wine = pd.read_csv('winequality-red.csv', sep = \";\", index_col = False)\n",
    "# shuffle dataframe\n",
    "df_wine = df_wine.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# define features and target labels\n",
    "features = list(df_wine.columns)[:-1]\n",
    "target = \"quality\"\n",
    "\n",
    "# calculate threshold of all qualities using median\n",
    "#threshold = df_wine[target].median()\n",
    "threshold = 7\n",
    "\n",
    "# Create a Tensorflow Dataset and a Dataset Pipeline\n",
    "## split data frame into train(60%), test(20%), validation(20%)\n",
    "train_df, test_df, val_df = np.split(df_wine.sample(frac=1, random_state=42), [int(0.6*len(df_wine)), int(0.8*len(df_wine))])\n",
    "\n",
    "## convert dataframes into tensorflow datasets\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (tf.cast(train_df[features].values, tf.float32), tf.cast(train_df[target].values, tf.int32))\n",
    "    )\n",
    ")\n",
    "\n",
    "test_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (tf.cast(test_df[features].values, tf.float32), tf.cast(test_df[target].values, tf.int32))\n",
    "    )\n",
    ")\n",
    "\n",
    "val_ds = ( tf.data.Dataset.from_tensor_slices(\n",
    "        (tf.cast(val_df[features].values, tf.float32), tf.cast(val_df[target].values, tf.int32))\n",
    "    )\n",
    ")\n",
    "\n",
    "# prepare datasets\n",
    "train_dataset = prepare_dataset(train_ds, threshold)\n",
    "\n",
    "test_dataset = prepare_dataset(test_ds, threshold)\n",
    "val_dataset = prepare_dataset(val_ds, threshold)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea5b278",
   "metadata": {},
   "source": [
    "## Training and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb6bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.0002\n",
    "    \n",
    "# Initialize the optimizers: SGD, Adam, RMSprop\n",
    "optimizers = [tf.keras.optimizers.SGD(learning_rate), tf.keras.optimizers.RMSprop(learning_rate) , tf.keras.optimizers.Adam(learning_rate) ]\n",
    "opt_labels = [\"SGD\", \"RMSprop\" , \"Adam\"]\n",
    "label = 0\n",
    "\n",
    "for opt in optimizers:\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Initialize the model.\n",
    "    model = MyModel()\n",
    "    # Initialize the loss: categorical cross entropy. Check out 'tf.keras.losses'.\n",
    "    b_cross_entropy_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "\n",
    "    # Initialize lists for later visualization.\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    \n",
    "    #testing once before we begin\n",
    "    test_loss, test_accuracy = test(model, test_dataset, b_cross_entropy_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    #check how model performs on train data once before we begin\n",
    "    train_loss, train_accuracy = test(model, train_dataset, b_cross_entropy_loss)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    #check how model performs on validate data once before we begin\n",
    "    val_loss, val_accuracy = test(model, val_dataset, b_cross_entropy_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    \n",
    "    # We train for num_epochs epochs\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        #training (and checking in with training)\n",
    "        epoch_loss_agg = []\n",
    "        epoch_acc_agg = []\n",
    "        for input,target in train_dataset:\n",
    "            train_loss, train_accuracy = train_step(model, input, target, b_cross_entropy_loss, opt)\n",
    "            epoch_loss_agg.append(train_loss)\n",
    "\n",
    "            #track training loss\n",
    "        train_losses.append(tf.reduce_mean(epoch_loss_agg))\n",
    "        # tracking train accuracy\n",
    "        train_accuracies.append(tf.reduce_mean(train_accuracy))\n",
    "        \n",
    "        #testing, so we can track accuracy and test loss\n",
    "        test_loss, test_accuracy = test(model, test_dataset, b_cross_entropy_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "        # validation\n",
    "        val_loss, val_accuracy = validate(model, val_dataset, b_cross_entropy_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        #print(f' test_losses: {test_losses[-1]} , test_accuracies: {test_accuracies[-1]}')\n",
    "        #print(f' val_losses: {val_losses[-1]}, test_accuracies: {test_accuracies[-1]}')\n",
    "        \n",
    "    print(opt_labels[label], \"optimizer\")\n",
    "    label+=1\n",
    "\n",
    "    plotting(train_losses, test_losses, val_losses, train_accuracies, test_accuracies, val_accuracies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
