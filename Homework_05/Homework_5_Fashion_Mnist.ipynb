{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cec64469",
   "metadata": {},
   "source": [
    "# Implementation of a CNN using fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477cb2f6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e45a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152e0d0c",
   "metadata": {},
   "source": [
    "## Input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0e6e07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description:Input pipeline to prepare dataset\n",
    "#             Inputs: fashion_mnist\n",
    "#             Outputs: prepared fashion_mnist\n",
    "def prepare_data(fashion_mnist):\n",
    "        \n",
    "    #convert datatype to float32 and normalize image pixels to range [-1, 1]\n",
    "    fashion_mnist = fashion_mnist.map(lambda img, target: ((tf.cast(img, tf.float32) / 128.0) - 1.0, target))\n",
    "            \n",
    "    #create one-hot targets\n",
    "    fashion_mnist = fashion_mnist.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
    "    #cache this progress in memory, as there is no need to redo it; it is deterministic after all\n",
    "    fashion_mnist = fashion_mnist.cache()\n",
    "    \n",
    "    #shuffle, batch, prefetch\n",
    "    fashion_mnist = fashion_mnist.shuffle(1000)\n",
    "    fashion_mnist = fashion_mnist.batch(128)\n",
    "    fashion_mnist = fashion_mnist.prefetch(150)\n",
    "    \n",
    "    #return preprocessed dataset\n",
    "    return fashion_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ef63a2",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05f26348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description: CNN Model with convolutional and pooling operations and classifier architecture \n",
    "#              since this is a classification task\n",
    "class CNNModel(tf.keras.Model):\n",
    "    \n",
    "    # Description: Set up layers\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.hidden_layer1 = tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu', kernel_regularizer= tf.keras.regularizers.l2(0.001), input_shape=(28, 28, 1))\n",
    "        self.hidden_layer2 = tf.keras.layers.MaxPool2D((2, 2), strides = 2)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(0.2)\n",
    "        \n",
    "        self.hidden_layer3 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu', kernel_regularizer= tf.keras.regularizers.l2(0.001))\n",
    "        # flatten input to process it with dense layer\n",
    "        self.flatten_layer = tf.keras.layers.Flatten()\n",
    "        self.hidden_layer4 = tf.keras.layers.Dense(128, activation='relu', kernel_regularizer= tf.keras.regularizers.l2(0.001))\n",
    "        self.dropout2 = tf.keras.layers.Dropout(0.5)\n",
    "        self.output_layer = tf.keras.layers.Dense(10, activation='softmax')\n",
    "     \n",
    "    \n",
    "    # Description: First a python decorater is called to transform the call function to a computational graph\n",
    "    #              Then the call function passes the inputs through the layers of the model (forward pass)\n",
    "    #              inputs: input\n",
    "    #              outputs: final_pass\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        x = self.hidden_layer1(inputs)\n",
    "        x = self.hidden_layer2(x) \n",
    "        x = self.dropout1(x)\n",
    "        x = self.hidden_layer3(x)\n",
    "        x = self.flatten_layer(x)\n",
    "        x = self.hidden_layer4(x) \n",
    "        x = self.dropout2(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330de6bb",
   "metadata": {},
   "source": [
    "## Training and testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a05b8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Description: This function trains an object of the class MyModel. It conducts a forward-step and the backpropagation \n",
    "#             throughout the network. The optimizer updates weights and biases. \n",
    "#             Inputs: model, inputs, target, loss function, optimizer\n",
    "#             Outputs:loss, accuracy\n",
    "def train_step(model, inputs, target, loss_function, optimizer):\n",
    "    # loss_object and optimizer_object are instances of respective tensorflow classes\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = model(inputs)\n",
    "        loss = loss_function(target, prediction)\n",
    "        sample_train_accuracy = np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
    "        sample_train_accuracy = np.mean(sample_train_accuracy)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss, sample_train_accuracy\n",
    "\n",
    "\n",
    "\n",
    "#Description: This function determines the test loss and test accuracy through a forward step in the network.\n",
    "#             Inputs: model, test data, loss function\n",
    "#             Outputs: test loss, test accuracy\n",
    "def test(model, test_data, loss_function):\n",
    "    test_accuracy_aggregator = []\n",
    "    test_loss_aggregator = []\n",
    "\n",
    "    for (input, target) in test_data:\n",
    "        prediction = model(input)        \n",
    "        sample_test_loss = loss_function(target, prediction)\n",
    "        sample_test_accuracy =  tf.equal(tf.argmax(prediction, 1), tf.argmax(target, 1))\n",
    "        sample_test_accuracy = np.mean(sample_test_accuracy)\n",
    "        test_loss_aggregator.append(sample_test_loss.numpy())\n",
    "        test_accuracy_aggregator.append(np.mean(sample_test_accuracy))\n",
    "\n",
    "    test_loss = tf.reduce_mean(test_loss_aggregator)\n",
    "    test_accuracy = tf.reduce_mean(test_accuracy_aggregator)\n",
    "\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9182b4b3",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7dd4fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Description: This function visualizes the losses and accuracies of training and testing \n",
    "#             Inputs: test,train,test,train, optimizer label\n",
    "#             Outputs: plot\n",
    "def plotting(train_losses, test_losses, train_accuracies, test_accuracies):\n",
    "    # plot losses\n",
    "    plt.figure()\n",
    "    line1, = plt.plot(train_losses)\n",
    "    line2, = plt.plot(test_losses)\n",
    "    plt.title(\"Loss with optimizer SGD\")\n",
    "    plt.xlabel(\"Training steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend((line1,line2),(\"Loss train\",\"Loss test\"))\n",
    "    plt.show()\n",
    "    \n",
    "    # plot accuracies\n",
    "    plt.figure()\n",
    "    line1, = plt.plot(train_accuracies)\n",
    "    line2, = plt.plot(test_accuracies)\n",
    "    plt.title(\"Accuracy with optimizer SGD\")\n",
    "    plt.xlabel(\"Training steps\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend((line1,line2),(\"Accuracy train\", \"Accuracy test\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c56824e",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213d69b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data\n",
    "train_ds, test_ds = tfds.load('fashion_mnist', split=['train', 'test'], shuffle_files=True, as_supervised=True)\n",
    "\n",
    "# prepare data: convert, hotify, shuffle, batch and prefetch\n",
    "train_dataset = prepare_data(train_ds)\n",
    "test_dataset = prepare_data(test_ds)\n",
    "\n",
    "# Training\n",
    "\n",
    "### Hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.1\n",
    "    \n",
    "# Initialize the optimizers: SGD\n",
    "opt = tf.keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Initialize the model.\n",
    "model = CNNModel()\n",
    "\n",
    "# Initialize the loss: categorical cross entropy. Check out 'tf.keras.losses'.\n",
    "c_cross_entropy_loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "# Initialize lists for later visualization.\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "#testing once before we begin\n",
    "test_loss, test_accuracy = test(model, test_dataset, c_cross_entropy_loss)\n",
    "test_losses.append(test_loss)\n",
    "test_accuracies.append(test_accuracy)\n",
    "\n",
    "#check how model performs on train data once before we begin\n",
    "train_loss, train_accuracy = test(model, train_dataset, c_cross_entropy_loss)\n",
    "train_losses.append(train_loss)\n",
    "train_accuracies.append(train_accuracy)\n",
    "\n",
    "# We train for num_epochs epochs\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #training (and checking in with training)\n",
    "    epoch_loss_agg = []\n",
    "    epoch_acc_agg = []\n",
    "    for input,target in train_dataset:\n",
    "        train_loss, train_accuracy = train_step(model, input, target, c_cross_entropy_loss, opt)\n",
    "        epoch_loss_agg.append(train_loss)\n",
    "\n",
    "    #track training loss\n",
    "    train_losses.append(tf.reduce_mean(epoch_loss_agg))\n",
    "    # tracking train accuracy\n",
    "    train_accuracies.append(tf.reduce_mean(train_accuracy))\n",
    "\n",
    "    #testing, so we can track accuracy and test loss\n",
    "    test_loss, test_accuracy = test(model, test_dataset, c_cross_entropy_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    print(f' test_losses: {test_losses[-1]} , test_accuracies: {test_accuracies[-1]}')\n",
    "\n",
    "# Visualization\n",
    "plotting(train_losses, test_losses, train_accuracies, test_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ecc134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba6e903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
